"""
√âvaluation du syst√®me anti-spam hybride 
Calcul des m√©triques: pr√©cision, faux positifs, faux n√©gatifs, etc.
"""

import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import pandas as pd

class SpamFilterEvaluator:
    def __init__(self, spam_filter):
        """
        Initialise l'√©valuateur
        
        Args:
            spam_filter: Instance de HybridSpamFilter
        """
        self.spam_filter = spam_filter
        self.results = None
    
    def evaluate(self, X_test, y_test, verbose=True):
        """
        √âvalue le syst√®me sur un jeu de test
        
        Args:
            X_test: Liste d'emails de test
            y_test: Labels r√©els (0=l√©gitime, 1=spam)
        
        Returns:
            Dict avec toutes les m√©triques
        """
        
        y_pred = []
        y_proba = []
        methods_used = []
        
        print(" Traitement des emails de test...")
        
        for i, email in enumerate(X_test):
            if verbose and i % 20 == 0:
                print(f"  {i}/{len(X_test)} emails trait√©s")
            
            result = self.spam_filter.classify(email)
            y_pred.append(1 if result['is_spam'] else 0)
            y_proba.append(result.get('ml_probability', 1.0 if result['is_spam'] else 0.0))
            methods_used.append(result['method'])
        
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
    
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
      
        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
        
   
        method_counts = pd.Series(methods_used).value_counts().to_dict()
        method_percentages = {
            method: (count / len(methods_used) * 100)
            for method, count in method_counts.items()
        }
        
        
        stats = self.spam_filter.get_statistics()
        
        self.results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'false_positive_rate': false_positive_rate,
            'false_negative_rate': false_negative_rate,
            'confusion_matrix': cm,
            'y_true': y_test,
            'y_pred': y_pred,
            'y_proba': y_proba,
            'methods_used': methods_used,
            'method_counts': method_counts,
            'method_percentages': method_percentages,
            'system_stats': stats,
            'test_size': len(X_test),
            'spam_ratio': sum(y_test) / len(y_test),
        }
        
        if verbose:
            self._print_results()
        
        return self.results
    
    def _print_results(self):
        """Affiche les r√©sultats de mani√®re lisible"""
        r = self.results
        
        print("  R√âSULTATS DE L'√âVALUATION DU SYST√àME ANTI-SPAM HYBRIDE")
        print("="*70 + "\n")
        
       
        print("M√âTRIQUES PRINCIPALES:")
        print(f"  ‚Ä¢ Pr√©cision globale (Accuracy): {r['accuracy']:.2%}")
        print(f"  ‚Ä¢ Pr√©cision (Precision):        {r['precision']:.2%}")
        print(f"  ‚Ä¢ Rappel (Recall):              {r['recall']:.2%}")
        print(f"  ‚Ä¢ F1-Score:                     {r['f1_score']:.2%}")
        
        
        print(f"\n OBJECTIFS DU PROJET:")
        fp_ok = "‚úÖ" if r['false_positive_rate'] < 0.01 else "‚ùå"
        recall_ok = "‚úÖ" if r['recall'] > 0.95 else "‚ùå"
        print(f"  {fp_ok} Faux positifs: {r['false_positive_rate']:.2%} (objectif: < 1%)")
        print(f"  {recall_ok} D√©tection spam: {r['recall']:.2%} (objectif: > 95%)")
        
     
        print(f"\n MATRICE DE CONFUSION:")
        print(f"  ‚Ä¢ Vrais positifs (spam d√©tect√©):      {r['true_positives']}")
        print(f"  ‚Ä¢ Vrais n√©gatifs (l√©gitime accept√©):  {r['true_negatives']}")
        print(f"  ‚Ä¢ Faux positifs (l√©gitime bloqu√©):    {r['false_positives']}")
        print(f"  ‚Ä¢ Faux n√©gatifs (spam non d√©tect√©):   {r['false_negatives']}")
        
        
        print(f"\n DISTRIBUTION DES M√âTHODES:")
        for method, count in r['method_counts'].items():
            percentage = r['method_percentages'][method]
            print(f"  ‚Ä¢ {method.upper():<12} {count:>4} emails ({percentage:5.1f}%)")
        
        
        stats = r['system_stats']
        print(f"\nüìä STATISTIQUES DU SYST√àME:")
        print(f"  ‚Ä¢Total trait√©: {stats['total_processed']}")
        print(f"  ‚Ä¢ Bloqu√© par r√®gles: {stats['blocked_by_rules']} ({stats.get('percentage_blocked_by_rules', 0):.1f}%)")
        print(f"  ‚Ä¢ Bloqu√© par ML: {stats['blocked_by_ml']} ({stats.get('percentage_blocked_by_ml', 0):.1f}%)")
        print(f"  ‚Ä¢ L√©gitime: {stats['legitimate']} ({stats.get('percentage_legitimate', 0):.1f}%)")
        
      
        if 'rule_triggers' in stats:
            print(f"\nüîß CONTRIBUTION DES R√àGLES HEURISTIQUES:")
            total_rules = sum(stats['rule_triggers'].values())
            if total_rules > 0:
                for rule_name, count in sorted(stats['rule_triggers'].items(), key=lambda x: x[1], reverse=True):
                    if count > 0:
                        percentage = (count / total_rules) * 100
                        print(f"  ‚Ä¢ {rule_name:<25} {count:>4} ({percentage:5.1f}%)")
        
        print("\n" + "="*70 + "\n")
    
    def generate_detailed_report(self, output_file='evaluation_report.txt'):
        """G√©n√®re un rapport d√©taill√© en format texte"""
        if self.results is None:
            raise Exception("Veuillez d'abord executer evaluate()")
        
        r = self.results
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("RAPPORT D'√âVALUATION - SYST√àME ANTI-SPAM HYBRIDE\n")
            f.write("="*80 + "\n\n")
            
            f.write("1. R√âSUM√â EX√âCUTIF\n")
            f.write("-" * 80 + "\n")
            f.write(f"Pr√©cision globale: {r['accuracy']:.2%}\n")
            f.write(f"Taux de faux positifs: {r['false_positive_rate']:.2%} ")
            f.write(f"({'OBJECTIF ATTEINT' if r['false_positive_rate'] < 0.01 else 'OBJECTIF NON ATTEINT'})\n")
            f.write(f"Taux de d√©tection spam: {r['recall']:.2%} ")
            f.write(f"({'OBJECTIF ATTEINT' if r['recall'] > 0.95 else 'OBJECTIF NON ATTEINT'})\n")
            f.write(f"Taille du jeu de test: {r['test_size']} emails\n")
            f.write(f"Ratio spam dans le test: {r['spam_ratio']:.1%}\n\n")
            
            f.write("2. M√âTRIQUES D√âTAILL√âES\n")
            f.write("-" * 80 + "\n")
            f.write(f"Pr√©cision: {r['precision']:.4f}\n")
            f.write(f"Recall: {r['recall']:.4f}\n")
            f.write(f"F1-Score: {r['f1_score']:.4f}\n")
            f.write(f"Vrais positifs: {r['true_positives']}\n")
            f.write(f"Vrais n√©gatifs: {r['true_negatives']}\n")
            f.write(f"Faux positifs: {r['false_positives']}\n")
            f.write(f"Faux n√©gatifs: {r['false_negatives']}\n\n")
            
            f.write("3. DISTRIBUTION DES M√âTHODES\n")
            f.write("-" * 80 + "\n")
            for method, count in r['method_counts'].items():
                percentage = r['method_percentages'][method]
                f.write(f"{method.upper():<12} {count:>6} emails ({percentage:6.2f}%)\n")
            f.write("\n")
            
            f.write("4. STATISTIQUES DU SYST√àME\n")
            f.write("-" * 80 + "\n")
            stats = r['system_stats']
            f.write(f"Total trait√©: {stats['total_processed']}\n")
            f.write(f"Bloqu√© par r√®gles: {stats['blocked_by_rules']}\n")
            f.write(f"Bloqu√© par ML: {stats['blocked_by_ml']}\n")
            f.write(f"Emails l√©gitimes: {stats['legitimate']}\n\n")
            
            if 'rule_triggers' in stats:
                f.write("5. CONTRIBUTION DES R√àGLES\n")
                f.write("-" * 80 + "\n")
                for rule_name, count in stats['rule_triggers'].items():
                    if count > 0:
                        f.write(f"{rule_name}: {count} d√©tections\n")
            
            f.write("\n" + "="*80 + "\n")
        
        print(f" Rapport d√©taill√© g√©n√©r√©: {output_file}")
    
    def find_false_positives(self, X_test, y_test, max_display=5):
        """Trouve et affiche les faux positifs (l√©gitimes bloqu√©s)"""
        if self.results is None:
            raise Exception("Veuillez d'abord executer evaluate()")
        
        false_positives = []
        
        for i, (email, true_label, pred_label) in enumerate(zip(X_test, y_test, self.results['y_pred'])):
            if true_label == 0 and pred_label == 1:  
                result = self.spam_filter.classify(email)
                false_positives.append({
                    'email': email,
                    'method': result['method'],
                    'reason': result['reason'],
                    'confidence': result['confidence']
                })
        
        if false_positives:
            print(f"\n FAUX POSITIFS D√âTECT√âS: {len(false_positives)}")
            print("-" * 80)
            
       
            method_counts = {}
            for fp in false_positives:
                method = fp['method']
                method_counts[method] = method_counts.get(method, 0) + 1
            
            print("Distribution par m√©thode:")
            for method, count in method_counts.items():
                percentage = (count / len(false_positives)) * 100
                print(f"  ‚Ä¢ {method.upper()}: {count} ({percentage:.1f}%)")
            
           
            print(f"\n Exemples (max {max_display}):")
            for i, fp in enumerate(false_positives[:max_display]):
                print(f"\n{i+1}. M√©thode: {fp['method'].upper()}")
                print(f"   Confiance: {fp['confidence']:.1%}")
                print(f"   Raison: {fp['reason']}")
                print(f"   Email: {fp['email'][:100]}...")
            
            if len(false_positives) > max_display:
                print(f"\n... et {len(false_positives) - max_display} autres faux positifs")
        else:
            print(f"\n AUCUN FAUX POSITIF D√âTECT√â !")
        
        return false_positives
    
    def get_recommendations(self):
        """Retourne des recommandations bas√©es sur les r√©sultats"""
        if self.results is None:
            raise Exception("Ex√©cutez evaluate() d'abord")
        
        r = self.results
        recommendations = []
        
       
        if r['false_positive_rate'] > 0.01:
            recommendations.append(
                " Augmenter le seuil ML (ex: 0.6 ou 0.7) pour r√©duire les faux positifs"
            )
            recommendations.append(
                " Ajouter des exceptions aux r√®gles heuristiques pour certains domaines l√©gitimes"
            )
            recommendations.append(
                " Analyser les faux positifs pour identifier les r√®gles trop strictes"
            )
        
        
        if r['recall'] < 0.95:
            recommendations.append(
                " Diminuer le seuil ML (ex: 0.4) pour d√©tecter plus de spams"
            )
            recommendations.append(
                " Ajouter de nouvelles r√®gles heuristiques bas√©es sur les faux n√©gatifs"
            )
            recommendations.append(
                " Enrichir le dataset d'entra√Ænement avec plus d'exemples de spam"
            )
        
        
        if r['method_percentages'].get('ml', 0) > 50:
            recommendations.append(
                " Optimiser les r√®gles heuristiques pour prendre en charge plus de cas √©vidents"
            )
        
        recommendations.append(
            " Consulter le rapport d√©taill√© pour une analyse compl√®te"
        )
        
        return recommendations
    
    def print_confusion_matrix_visual(self):
        """Affiche une version visuelle de la matrice de confusion"""
        if self.results is None:
            raise Exception("Ex√©cutez evaluate() d'abord")
        
        r = self.results
        cm = r['confusion_matrix']
        
        print("\n MATRICE DE CONFUSION (visuelle):")
        print("-" * 50)
        print("              PR√âDIT")
        print("           L√©gitime   Spam")
        print(f"R√âEL   L√©gitime  {cm[0,0]:^6}   {cm[0,1]:^6}")
        print(f"       Spam       {cm[1,0]:^6}   {cm[1,1]:^6}")
        print("-" * 50)
        
        print("\n L√âGENDE:")
        print(f"  {cm[0,0]} = Vrais n√©gatifs (l√©gitimes correctement accept√©s)")
        print(f"  {cm[0,1]} = Faux positifs (l√©gitimes incorrectement bloqu√©s)")
        print(f"  {cm[1,0]} = Faux n√©gatifs (spams manqu√©s)")
        print(f"  {cm[1,1]} = Vrais positifs (spams correctement bloqu√©s)")
    
    def analyze_phishing_detection(self, X_test, y_test):
   
     if self.results is None:
        raise Exception("Ex√©cutez evaluate() d'abord")
    
     
     phishing_indices = []
     for i, email in enumerate(X_test):
        email_lower = email.lower()
       
        phishing_indicators = [
            'security review', 'v√©rifications r√©guli√®res',
            'access my account', 'acc√©der √† mon espace',
            'temporarily unavailable', 'limitation temporaire',
            'üëâ', 'click here', 'cliquez ici'
        ]
        
        if any(indicator in email_lower for indicator in phishing_indicators):
            phishing_indices.append(i)
    
     if not phishing_indices:
        print(" Aucun email de phishing identifi√© dans le jeu de test")
        return
    
     print(f"\n ANALYSE D√âTECTION PHISHING: {len(phishing_indices)} emails identifi√©s")
     print("-" * 80)
    
    
     true_positives = 0
     false_negatives = 0
    
     for idx in phishing_indices:
        is_phishing = y_test[idx] == 1
        predicted_spam = self.results['y_pred'][idx] == 1
        
        if is_phishing and predicted_spam:
            true_positives += 1
        elif is_phishing and not predicted_spam:
            false_negatives += 1
            
            if false_negatives <= 3:  
                print(f"‚ùå Faux n√©gatif phishing #{false_negatives}:")
                print(f"   Email: {X_test[idx][:100]}...")
                print(f"   M√©thode: {self.results['methods_used'][idx]}")
                print(f"   Probabilit√© ML: {self.results['y_proba'][idx]:.1%}")
                print()
  
 
     if len(phishing_indices) > 0:
        phishing_recall = true_positives / len(phishing_indices)
        print(f" PERFORMANCE PHISHING:")
        print(f"   ‚Ä¢ Vrais positifs: {true_positives}")
        print(f"   ‚Ä¢ Faux n√©gatifs: {false_negatives}")
        print(f"   ‚Ä¢ Rappel phishing: {phishing_recall:.1%}")
        
      
        if phishing_recall < 0.9:
            print(f"\n RECOMMANDATIONS PHISHING:")
            print("   1. Ajouter plus d'exemples de phishing dans le dataset")
            print("   2. Renforcer les r√®gles heuristiques de phishing")
            print("   3. Baisser le seuil ML pour les emails suspects")
        
        return {
            'phishing_count': len(phishing_indices),
            'true_positives': true_positives,
            'false_negatives': false_negatives,
            'recall': phishing_recall
        }


if __name__ == "__main__":
    print(" Module d'√©valuation charg√©.")
    print("\n Utilisation:")
    print("  evaluator = SpamFilterEvaluator(spam_filter)")
    print("  results = evaluator.evaluate(X_test, y_test)")
    print("  evaluator.generate_detailed_report('rapport.txt')")
    print("  evaluator.find_false_positives(X_test, y_test)")
    print("  evaluator.get_recommendations()")
